{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            ride_id                         driver_id  \\\n",
      "0  006d61cf7446e682f7bc50b0f8a5bea5  002be0ffdc997bd5c50703158b7c2491   \n",
      "1  01b522c5c3a756fbdb12e95e87507eda  002be0ffdc997bd5c50703158b7c2491   \n",
      "2  029227c4c2971ce69ff2274dc798ef43  002be0ffdc997bd5c50703158b7c2491   \n",
      "3  034e861343a63ac3c18a9ceb1ce0ac69  002be0ffdc997bd5c50703158b7c2491   \n",
      "4  034f2e614a2f9fc7f1c2f77647d1b981  002be0ffdc997bd5c50703158b7c2491   \n",
      "\n",
      "   ride_duration  ride_distance  ride_prime_time  accept_delay  completed_flag  \n",
      "0            327           1811               50          25.0               1  \n",
      "1            809           3362                0           3.0               1  \n",
      "2            572           3282                0           8.0               1  \n",
      "3           3338          65283               25           4.0               1  \n",
      "4            823           4115              100           2.0               1  \n"
     ]
    }
   ],
   "source": [
    "'''Converting to ipynb to py in the end using:\n",
    "jupyter nbconvert --to script rating.ipynb'''\n",
    "\n",
    "'''https://github.com/kevinchen27/lyft-rides-analysis/blob/master/README.md'''\n",
    "\n",
    "'''Load and Merge the Dataset'''\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Load config\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load data using config paths\n",
    "df_drivers = pd.read_csv(config['data_paths']['driver_ids'])\n",
    "df_rides = pd.read_csv(config['data_paths']['ride_ids'])\n",
    "df_timestamps = pd.read_csv(config['data_paths']['ride_timestamps'])\n",
    "\n",
    "\n",
    "# Pivot the timestamp events to wide format: one row per ride_id with columns for each event\n",
    "df_events = df_timestamps.pivot(index='ride_id', columns='event', values='timestamp').reset_index()\n",
    "\n",
    "# Convert timestamp columns to datetime\n",
    "for col in ['requested', 'accepted', 'completed']:\n",
    "    if col in df_events.columns:\n",
    "        df_events[col] = pd.to_datetime(df_events[col])\n",
    "\n",
    "# Joining data on ride_id to obtain per-ride data\n",
    "\n",
    "# Join ride data with event timestamps\n",
    "df_combined = pd.merge(df_rides, df_events, on='ride_id', how='inner')\n",
    "\n",
    "# Join driver info (e.g., onboarding date)\n",
    "df_combined = pd.merge(df_combined, df_drivers, on='driver_id', how='left')\n",
    "\n",
    "# Compute accept delay (seconds)\n",
    "df_combined['accept_delay'] = (pd.to_datetime(df_combined['accepted_at']) - pd.to_datetime(df_combined['requested_at'])).dt.total_seconds()\n",
    "\n",
    "# Compute completion flag: 1 if the ride was dropped off\n",
    "df_combined['completed_flag'] = df_combined['dropped_off_at'].notnull().astype(int)\n",
    "\n",
    "# Preview\n",
    "print(df_combined[['ride_id', 'driver_id', 'ride_duration', 'ride_distance', 'ride_prime_time', 'accept_delay', 'completed_flag']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "\n",
    "## Step 1: Load and Merge the Dataset\n",
    "\n",
    "- Load the following CSV files:\n",
    "  - `driver_ids.csv`: Contains driver onboarding information.\n",
    "  - `ride_ids.csv`: Includes ride duration, distance, prime time multiplier, and driver ID.\n",
    "  - `ride_timestamps.csv`: Contains ride lifecycle events (e.g., requested, accepted, completed).\n",
    "- Join all three datasets on `ride_id` to create a unified view of each ride.\n",
    "\n",
    "## Step 2: Create Per-Ride and Per-Driver Features\n",
    "\n",
    "- From `ride_timestamps.csv`:\n",
    "  - Calculate **Accept Delay**: Time between request and accept.\n",
    "  - Calculate **Ride Completion Rate**: Percentage of requests that reach the \"completed\" event.\n",
    "  - Simulate cancellations as rides that were accepted but not completed.\n",
    "\n",
    "- From `ride_ids.csv`:\n",
    "  - Calculate **Average Trip Duration** and **Average Distance**.\n",
    "  - Determine **Prime Time Utilization**: Percentage of rides with a high prime time multiplier (e.g., > 1.5).\n",
    "\n",
    "## Step 3: Aggregate Features Per Driver\n",
    "\n",
    "- For each driver, compute aggregated performance metrics:\n",
    "  - Average trip duration.\n",
    "  - Average accept delay.\n",
    "  - Completion rate.\n",
    "  - Percentage of rides in high prime time.\n",
    "  - Total number of rides.\n",
    "\n",
    "## Step 4: Label Drivers\n",
    "\n",
    "- Assign each driver to a rating category based on performance:\n",
    "  - **Poor**: Low completion rate or high average accept delay.\n",
    "  - **Average**: Slightly better metrics but not outstanding.\n",
    "  - **Good**: High completion rate with acceptable response times.\n",
    "  - **Excellent**: Very high performance across the board.\n",
    "- Customize thresholds based on data distribution.\n",
    "\n",
    "## Step 5: Train a Multi-Class Classifier\n",
    "\n",
    "- Use the labeled dataset to train a model to predict driver ratings.\n",
    "- Consider algorithms such as:\n",
    "  - XGBoost (high performance for tabular data).\n",
    "  - Random Forest.\n",
    "  - Logistic Regression (in multi-class mode).\n",
    "- Evaluate performance using metrics like:\n",
    "  - Accuracy\n",
    "  - F1-score\n",
    "  - Confusion matrix\n",
    "\n",
    "## Optional Improvements\n",
    "\n",
    "- Incorporate time-based features like:\n",
    "  - Ride frequency by weekday.\n",
    "  - Peak hour patterns.\n",
    "- Apply feature scaling as needed.\n",
    "- Use hyperparameter tuning (e.g., GridSearchCV) to improve model performance.\n",
    "\n",
    "\n",
    "## Why we Still Need a Model\n",
    "Even though you're assigning labels:\n",
    "The model learns the underlying patterns in the data.\n",
    "\n",
    "It can predict new driver ratings without hardcoded thresholds.\n",
    "\n",
    "It’s more flexible than rules — especially as your dataset grows.\n",
    "\n",
    "You can run it off-chain and send the result back to the smart contract as the driver's updated score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
